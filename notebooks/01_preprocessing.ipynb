{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "379de52a",
   "metadata": {},
   "source": [
    "# 01: Data Preprocessing\n",
    "\n",
    "This notebook handles cleaning, missing values, outlier detection, and aggregation of NPS and revenue data from healthcare clinics.\n",
    "\n",
    "**Dataset**: 27,000+ NPS survey responses across 36 months (2022-2025)  \n",
    "**Compliance**: Fully anonymized and LGPD/GDPR compliant\n",
    "\n",
    "## Research Question\n",
    "Does NPS truly predict revenue in healthcare clinics? This preprocessing step prepares the data for rigorous statistical testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb6108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2e59db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d763fdcf",
   "metadata": {},
   "source": [
    "## Load Healthcare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b52b497",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw/nps/nps_responses.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load NPS and Revenue data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m nps_data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/raw/nps/nps_responses.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m revenue_data = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mdata/raw/sales/revenue_by_clinic.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mNPS Dataset Shape:\u001b[39m\u001b[33m'\u001b[39m, nps_data.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/raw/nps/nps_responses.csv'"
     ]
    }
   ],
   "source": [
    "# Load NPS and Revenue data\n",
    "nps_data = pd.read_csv('data/raw/nps/nps_responses.csv')\n",
    "revenue_data = pd.read_csv('data/raw/sales/revenue_by_clinic.csv')\n",
    "\n",
    "print('NPS Dataset Shape:', nps_data.shape)\n",
    "print('\\nNPS Dataset Info:')\n",
    "print(nps_data.info())\n",
    "print('\\nFirst few NPS records:')\n",
    "print(nps_data.head())\n",
    "print('\\n' + '='*60)\n",
    "print('Revenue Dataset Shape:', revenue_data.shape)\n",
    "print('\\nRevenue Dataset Info:')\n",
    "print(revenue_data.info())\n",
    "print('\\nFirst few revenue records:')\n",
    "print(revenue_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c826728",
   "metadata": {},
   "source": [
    "## Handle Missing Values\n",
    "\n",
    "In healthcare data, missing values require careful handling. We use statistical methods:\n",
    "- **Continuous variables**: Median imputation (robust to outliers)\n",
    "- **Time series**: Forward/backward fill for revenue\n",
    "- **Categorical**: Mode or 'Unknown' flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca33768d",
   "metadata": {},
   "source": [
    "## Data Type Conversion\n",
    "\n",
    "Ensure all columns have appropriate data types for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d930678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime\n",
    "date_col = 'date' if 'date' in nps_data.columns else 'Date'\n",
    "if date_col in nps_data.columns:\n",
    "    nps_data[date_col] = pd.to_datetime(nps_data[date_col])\n",
    "    \n",
    "date_col_rev = 'date' if 'date' in revenue_data.columns else 'Date'\n",
    "if date_col_rev in revenue_data.columns:\n",
    "    revenue_data[date_col_rev] = pd.to_datetime(revenue_data[date_col_rev])\n",
    "\n",
    "# Ensure NPS scores are numeric\n",
    "if 'nps_score' in nps_data.columns:\n",
    "    nps_data['nps_score'] = pd.to_numeric(nps_data['nps_score'], errors='coerce')\n",
    "\n",
    "# Ensure revenue is numeric\n",
    "if 'revenue' in revenue_data.columns:\n",
    "    revenue_data['revenue'] = pd.to_numeric(revenue_data['revenue'], errors='coerce')\n",
    "\n",
    "print('Data types after conversion:')\n",
    "print('\\nNPS Data:')\n",
    "print(nps_data.dtypes)\n",
    "print('\\nRevenue Data:')\n",
    "print(revenue_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649dc48b",
   "metadata": {},
   "source": [
    "## Outlier Detection and Treatment\n",
    "\n",
    "Using statistical methods (IQR for NPS, Z-score for revenue) to identify anomalies without removing data—healthcare data is often sparse and valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0408c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers in NPS scores using IQR method\n",
    "if 'nps_score' in nps_data.columns:\n",
    "    Q1 = nps_data['nps_score'].quantile(0.25)\n",
    "    Q3 = nps_data['nps_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_nps = nps_data[(nps_data['nps_score'] < lower_bound) | (nps_data['nps_score'] > upper_bound)]\n",
    "    print(f'NPS Outliers detected: {len(outliers_nps)} out of {len(nps_data)} ({100*len(outliers_nps)/len(nps_data):.2f}%)')\n",
    "    print(f'Bounds: [{lower_bound:.2f}, {upper_bound:.2f}]')\n",
    "    \n",
    "    # Mark outliers rather than remove them\n",
    "    nps_data['nps_is_outlier'] = (nps_data['nps_score'] < lower_bound) | (nps_data['nps_score'] > upper_bound)\n",
    "\n",
    "# Detect outliers in Revenue using Z-score method\n",
    "if 'revenue' in revenue_data.columns:\n",
    "    z_scores = np.abs(stats.zscore(revenue_data['revenue'].dropna()))\n",
    "    threshold = 3\n",
    "    revenue_data['revenue_zscore'] = np.abs(stats.zscore(revenue_data['revenue'].fillna(revenue_data['revenue'].mean())))\n",
    "    outliers_revenue = revenue_data[revenue_data['revenue_zscore'] > threshold]\n",
    "    print(f'\\nRevenue Outliers detected (Z-score > {threshold}): {len(outliers_revenue)} out of {len(revenue_data)} ({100*len(outliers_revenue)/len(revenue_data):.2f}%)')\n",
    "    \n",
    "    # Mark outliers\n",
    "    revenue_data['revenue_is_outlier'] = revenue_data['revenue_zscore'] > threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d7b8e3",
   "metadata": {},
   "source": [
    "## Aggregation by Clinic and Month\n",
    "\n",
    "Convert individual survey responses into clinic-month aggregates with summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ca9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year-month from date columns\n",
    "date_col = 'date' if 'date' in nps_data.columns else 'Date'\n",
    "nps_data['year_month'] = nps_data[date_col].dt.to_period('M')\n",
    "\n",
    "date_col_rev = 'date' if 'date' in revenue_data.columns else 'Date'\n",
    "revenue_data['year_month'] = revenue_data[date_col_rev].dt.to_period('M')\n",
    "\n",
    "# Aggregate NPS by clinic and month\n",
    "clinic_col = 'clinic_id' if 'clinic_id' in nps_data.columns else 'clinic'\n",
    "nps_agg = nps_data.groupby([clinic_col, 'year_month']).agg({\n",
    "    'nps_score': ['mean', 'median', 'std', 'count'],\n",
    "    'nps_is_outlier': 'sum' if 'nps_is_outlier' in nps_data.columns else 'count',\n",
    "    date_col: 'first'\n",
    "}).reset_index()\n",
    "\n",
    "nps_agg.columns = ['clinic_id', 'year_month', 'nps_mean', 'nps_median', 'nps_std', 'nps_responses', 'nps_outliers', 'date']\n",
    "\n",
    "print('Aggregated NPS Data (by clinic-month):')\n",
    "print(nps_agg.head(10))\n",
    "print(f'\\nShape: {nps_agg.shape}')\n",
    "print(f'\\nClinics covered: {nps_agg[\"clinic_id\"].nunique()}')\n",
    "print(f'Months covered: {nps_agg[\"year_month\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5bddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Revenue by clinic and month\n",
    "clinic_col_rev = 'clinic_id' if 'clinic_id' in revenue_data.columns else 'clinic'\n",
    "revenue_agg = revenue_data.groupby([clinic_col_rev, 'year_month']).agg({\n",
    "    'revenue': ['sum', 'mean', 'std'],\n",
    "    'revenue_is_outlier': 'sum' if 'revenue_is_outlier' in revenue_data.columns else 'count',\n",
    "    date_col_rev: 'first'\n",
    "}).reset_index()\n",
    "\n",
    "revenue_agg.columns = ['clinic_id', 'year_month', 'revenue_total', 'revenue_mean', 'revenue_std', 'revenue_outliers', 'date']\n",
    "\n",
    "print('Aggregated Revenue Data (by clinic-month):')\n",
    "print(revenue_agg.head(10))\n",
    "print(f'\\nShape: {revenue_agg.shape}')\n",
    "print(f'Clinics covered: {revenue_agg[\"clinic_id\"].nunique()}')\n",
    "print(f'Months covered: {revenue_agg[\"year_month\"].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf6d1e8",
   "metadata": {},
   "source": [
    "## Create Derived Metrics\n",
    "\n",
    "Calculate month-over-month changes and lagged features for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced2864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge NPS and Revenue data\n",
    "combined = nps_agg.merge(revenue_agg, on=['clinic_id', 'year_month'], how='inner', suffixes=('_nps', '_revenue'))\n",
    "\n",
    "# Sort by clinic and date\n",
    "combined = combined.sort_values(['clinic_id', 'date_nps'])\n",
    "\n",
    "# Calculate month-over-month changes\n",
    "combined['nps_change'] = combined.groupby('clinic_id')['nps_mean'].pct_change() * 100\n",
    "combined['revenue_change'] = combined.groupby('clinic_id')['revenue_total'].pct_change() * 100\n",
    "\n",
    "# Calculate lagged NPS (NPS from previous month)\n",
    "combined['nps_lag1'] = combined.groupby('clinic_id')['nps_mean'].shift(1)\n",
    "combined['nps_lag3'] = combined.groupby('clinic_id')['nps_mean'].shift(3)\n",
    "\n",
    "# Calculate revenue_per_respondent (efficiency metric)\n",
    "combined['revenue_per_response'] = combined['revenue_total'] / combined['nps_responses']\n",
    "\n",
    "print('Combined and enriched dataset:')\n",
    "print(combined.head(15))\n",
    "print(f'\\nShape: {combined.shape}')\n",
    "print(f'Columns: {combined.columns.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ed795",
   "metadata": {},
   "source": [
    "## Data Validation and Quality Checks\n",
    "\n",
    "Final validation before analysis: check for duplicates, logical consistency, and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f4237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "before_dedup = len(combined)\n",
    "combined = combined.drop_duplicates(subset=['clinic_id', 'year_month'])\n",
    "after_dedup = len(combined)\n",
    "print(f'Duplicates removed: {before_dedup - after_dedup}')\n",
    "\n",
    "# Logical consistency checks\n",
    "print('\\nLogical Consistency Checks:')\n",
    "print(f'NPS scores in range [0, 100]: {combined[\"nps_mean\"].between(0, 100).sum()}/{len(combined)}')\n",
    "print(f'Revenue values positive: {(combined[\"revenue_total\"] > 0).sum()}/{len(combined)}')\n",
    "print(f'Rows with valid lagged NPS: {combined[\"nps_lag1\"].notna().sum()}/{len(combined)}')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('Summary Statistics:')\n",
    "summary_cols = ['nps_mean', 'nps_std', 'revenue_total', 'nps_change', 'revenue_change', 'revenue_per_response']\n",
    "print(combined[summary_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e55ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "combined.to_csv('../data/processed/combined_nps_revenue.csv', index=False)\n",
    "print('✓ Processed data saved to: ../data/processed/combined_nps_revenue.csv')\n",
    "print(f'✓ Final dataset: {combined.shape[0]} clinic-months, {combined.shape[1]} features')\n",
    "print(f'✓ Time span: {combined[\"year_month\"].min()} to {combined[\"year_month\"].max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efdd94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(654, 4)\n",
      "(104, 4)\n"
     ]
    }
   ],
   "source": [
    "# Identify missing values\n",
    "print('Missing Values in NPS Data:')\n",
    "missing_nps = nps_data.isnull().sum()\n",
    "print(missing_nps[missing_nps > 0] if missing_nps.sum() > 0 else 'No missing values')\n",
    "print(f'Total missing: {missing_nps.sum()}')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('Missing Values in Revenue Data:')\n",
    "missing_revenue = revenue_data.isnull().sum()\n",
    "print(missing_revenue[missing_revenue > 0] if missing_revenue.sum() > 0 else 'No missing values')\n",
    "print(f'Total missing: {missing_revenue.sum()}')\n",
    "\n",
    "# Handle missing values in NPS data\n",
    "if 'nps_score' in nps_data.columns:\n",
    "    nps_data['nps_score'].fillna(nps_data['nps_score'].median(), inplace=True)\n",
    "\n",
    "# For categorical data: use mode or 'Unknown'\n",
    "for col in nps_data.select_dtypes(include='object').columns:\n",
    "    if nps_data[col].isnull().any():\n",
    "        nps_data[col].fillna(nps_data[col].mode()[0] if len(nps_data[col].mode()) > 0 else 'Unknown', inplace=True)\n",
    "\n",
    "# Handle missing values in Revenue data - sort first, then fill\n",
    "revenue_data = revenue_data.sort_values('date' if 'date' in revenue_data.columns else 'Date')\n",
    "if 'revenue' in revenue_data.columns:\n",
    "    revenue_data['revenue'].fillna(method='ffill', inplace=True)\n",
    "    revenue_data['revenue'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "print('\\nMissing values after imputation:')\n",
    "print(f'NPS: {nps_data.isnull().sum().sum()}')\n",
    "print(f'Revenue: {revenue_data.isnull().sum().sum()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
